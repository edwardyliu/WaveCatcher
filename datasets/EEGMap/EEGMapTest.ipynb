{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import time\n",
    "from functools import reduce\n",
    "import math as m\n",
    "\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy.io\n",
    "from scipy.interpolate import griddata\n",
    "import scipy.stats as scs\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "import re\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_images(locs, features, n_gridpoints, normalize=True,\n",
    "               augment=False, pca=False, std_mult=0.1, n_components=2, edgeless=False):\n",
    "    \"\"\"\n",
    "    Generates EEG images given electrode locations in 2D space and multiple feature values for each electrode\n",
    "\n",
    "    :param locs: An array with shape [n_electrodes, 2] containing X, Y\n",
    "                        coordinates for each electrode.\n",
    "    :param features: Feature matrix as [n_samples, n_features]\n",
    "                                Features are as columns.\n",
    "                                Features corresponding to each frequency band are concatenated.\n",
    "                                (alpha1, alpha2, ..., beta1, beta2,...)\n",
    "    :param n_gridpoints: Number of pixels in the output images\n",
    "    :param normalize:   Flag for whether to normalize each band over all samples\n",
    "    :param augment:     Flag for generating augmented images\n",
    "    :param pca:         Flag for PCA based data augmentation\n",
    "    :param std_mult     Multiplier for std of added noise\n",
    "    :param n_components: Number of components in PCA to retain for augmentation\n",
    "    :param edgeless:    If True generates edgeless images by adding artificial channels\n",
    "                        at four corners of the image with value = 0 (default=False).\n",
    "    :return:            Tensor of size [samples, colors, W, H] containing generated\n",
    "                        images.\n",
    "    \"\"\"\n",
    "    feat_array_temp = []\n",
    "    nElectrodes = locs.shape[0]     # Number of electrodes\n",
    "    # Test whether the feature vector length is divisible by number of electrodes\n",
    "    assert features.shape[1] % nElectrodes == 0\n",
    "    n_colors = features.shape[1] // nElectrodes\n",
    "    for c in range(int(n_colors)):\n",
    "        feat_array_temp.append(features[:, c * nElectrodes : nElectrodes * (c+1)])\n",
    "    if augment:\n",
    "        if pca:\n",
    "            for c in range(n_colors):\n",
    "                feat_array_temp[c] = augment_EEG(feat_array_temp[c], std_mult, pca=True, n_components=n_components)\n",
    "        else:\n",
    "            for c in range(n_colors):\n",
    "                feat_array_temp[c] = augment_EEG(feat_array_temp[c], std_mult, pca=False, n_components=n_components)\n",
    "    nSamples = features.shape[0]\n",
    "    # Interpolate the values\n",
    "    grid_x, grid_y = np.mgrid[\n",
    "                     min(locs[:, 0]):max(locs[:, 0]):n_gridpoints*1j,\n",
    "                     min(locs[:, 1]):max(locs[:, 1]):n_gridpoints*1j\n",
    "                     ]\n",
    "    temp_interp = []\n",
    "    for c in range(n_colors):\n",
    "        temp_interp.append(np.zeros([nSamples, n_gridpoints, n_gridpoints]))\n",
    "    # Generate edgeless images\n",
    "    if edgeless:\n",
    "        min_x, min_y = np.min(locs, axis=0)\n",
    "        max_x, max_y = np.max(locs, axis=0)\n",
    "        locs = np.append(locs, np.array([[min_x, min_y], [min_x, max_y],[max_x, min_y],[max_x, max_y]]),axis=0)\n",
    "        for c in range(n_colors):\n",
    "            feat_array_temp[c] = np.append(feat_array_temp[c], np.zeros((nSamples, 4)), axis=1)\n",
    "    # Interpolating\n",
    "    for i in range(nSamples):\n",
    "        for c in range(n_colors):\n",
    "            temp_interp[c][i, :, :] = griddata(locs, feat_array_temp[c][i, :], (grid_x, grid_y),\n",
    "                                    method='cubic', fill_value=np.nan)\n",
    "        print('Interpolating {0}/{1}\\r'.format(i+1, nSamples), end='\\r')\n",
    "    # Normalizing\n",
    "    for c in range(n_colors):\n",
    "        if normalize:\n",
    "            temp_interp[c][~np.isnan(temp_interp[c])] = \\\n",
    "                scale(temp_interp[c][~np.isnan(temp_interp[c])])\n",
    "        temp_interp[c] = np.nan_to_num(temp_interp[c])\n",
    "    return np.swapaxes(np.asarray(temp_interp), 0, 1)     # swap axes to have [samples, colors, W, H]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fft(snippet):\n",
    "    Fs = 128.0;  # sampling rate\n",
    "    snippet_time = len(snippet)/Fs\n",
    "    Ts = 1.0/Fs; # sampling interval\n",
    "    t = np.arange(0,snippet_time,Ts) # time vector\n",
    "\n",
    "    y = snippet\n",
    "    n = len(y) # length of the signal\n",
    "    k = np.arange(n)\n",
    "    T = n/Fs\n",
    "    frq = k/T # two sides frequency range\n",
    "    frq = frq[range(n//2)] # one side frequency range\n",
    "\n",
    "    Y = np.fft.fft(y)/n # fft computing and normalization\n",
    "    Y = Y[range(n//2)]\n",
    "    return frq,abs(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def theta_alpha_beta_averages(f,Y):\n",
    "    theta_range = (4,8)\n",
    "    alpha_range = (8,12)\n",
    "    beta_range = (12,32)\n",
    "    theta = Y[(f>=theta_range[0]) & (f<=theta_range[1])].mean()\n",
    "    alpha = Y[(f>=alpha_range[0]) & (f<=alpha_range[1])].mean()\n",
    "    beta = Y[(f>=beta_range[0]) & (f<=beta_range[1])].mean()\n",
    "    return theta, alpha, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_steps(samples,frame_duration,overlap):\n",
    "    '''\n",
    "    in:\n",
    "    samples - number of samples in the session\n",
    "    frame_duration - frame duration in seconds \n",
    "    overlap - float fraction of frame to overlap in range (0,1)\n",
    "    \n",
    "    out: list of tuple ranges\n",
    "    '''\n",
    "    #steps = np.arange(0,len(df),frame_length)\n",
    "    Fs = 128\n",
    "    i = 0\n",
    "    intervals = []\n",
    "    samples_per_frame = Fs * frame_duration\n",
    "    while i+samples_per_frame <= samples:\n",
    "        intervals.append((i,i+samples_per_frame))\n",
    "        i = i + samples_per_frame - int(samples_per_frame*overlap)\n",
    "    return intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_frames(df,frame_duration):\n",
    "    '''\n",
    "    in: dataframe or array with all channels, frame duration in seconds\n",
    "    out: array of theta, alpha, beta averages for each probe for each time step\n",
    "        shape: (n-frames,m-probes,k-brainwave bands)\n",
    "    '''\n",
    "    Fs = 128.0\n",
    "    frame_length = Fs*frame_duration\n",
    "    frames = []\n",
    "    steps = make_steps(len(df),frame_duration,overlap)\n",
    "    for i,_ in enumerate(steps):\n",
    "        frame = []\n",
    "        if i == 0:\n",
    "            continue\n",
    "        else:\n",
    "            for channel in df.columns:\n",
    "                snippet = np.array(df.loc[steps[i][0]:steps[i][1],int(channel)])\n",
    "                f,Y =  get_fft(snippet)\n",
    "                theta, alpha, beta = theta_alpha_beta_averages(f,Y)\n",
    "                frame.append([theta, alpha, beta])\n",
    "            \n",
    "        frames.append(frame)\n",
    "    return np.array(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "locs_2d = [(-2.0,4.0),\n",
    "           (2.0,4.0),\n",
    "           (-1.0,3.0),\n",
    "           (1.0,3.0),\n",
    "           (-3.0,3.0),\n",
    "           (3.0,3.0),\n",
    "           (-2.0,2.0),\n",
    "           (2.0,2.0),\n",
    "           (-2.0,-2.0),\n",
    "           (2.0,-2.0),\n",
    "           (-4.0,1.0),\n",
    "           (4.0,1.0),\n",
    "           (-1.0,-3.0),\n",
    "           (1.0,-3.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_pipeline(file_names,labels,image_size,frame_duration,overlap):\n",
    "    '''\n",
    "    IN: \n",
    "    file_names - list of strings for each input file (one for each subject)\n",
    "    labels - list of labels for each\n",
    "    image_size - int size of output images in form (x, x)\n",
    "    frame_duration - time length of each frame (seconds)\n",
    "    overlap - float fraction of frame to overlap in range (0,1)\n",
    "    \n",
    "    OUT:\n",
    "    X: np array of frames (unshuffled)\n",
    "    y: np array of label for each frame (1 or 0)\n",
    "    '''\n",
    "    ##################################\n",
    "    ###Still need to do the overlap###!!!\n",
    "    ##################################\n",
    "    \n",
    "    Fs = 256.0   #sampling rate\n",
    "    frame_length = Fs * frame_duration\n",
    "    \n",
    "    print('Generating training data...')\n",
    "\n",
    "    trim = 5\n",
    "    \n",
    "    for i, file in enumerate(file_names):\n",
    "        print ('Processing session: ',file, '. (',i+1,' of ',len(file_names),')')\n",
    "        data = genfromtxt(file, delimiter=',').T\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        \"\"\"\n",
    "        df = pd.read_csv(file)\n",
    "        df = df.loc[:, ['TP9', 'AF7', 'AF8', 'TP10']]\n",
    "        df = df.truncate(before=trim*Fs, after=df.shape[0]-trim*Fs-1)\n",
    "        buffer = df.values\n",
    "        print(print(buffer))\n",
    "        df = pd.DataFrame(buffer)\n",
    "        print(df.shape)\n",
    "        \"\"\"\n",
    "        \n",
    "        X_0 = make_frames(df,frame_duration)\n",
    "        #steps = np.arange(0,len(df),frame_length)\n",
    "        X_1 = X_0.reshape(len(X_0),14*3) # 14 electrodes for test dataset\n",
    "        \n",
    "        images = gen_images(np.array(locs_2d), X_1, image_size, normalize=False)\n",
    "        images = np.swapaxes(images, 1, 3) \n",
    "        print(len(images), ' frames generated with label ', labels[i], '.')\n",
    "        print('\\n')\n",
    "        if i == 0:\n",
    "            X = images\n",
    "            y = np.ones(len(images))*labels[0]\n",
    "        else:\n",
    "            X = np.concatenate((X,images),axis = 0)\n",
    "            y = np.concatenate((y,np.ones(len(images))*labels[i]),axis = 0)\n",
    "        \n",
    "        \n",
    "    return X,np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating training data...\n",
      "Processing session:  EEGMapTest.csv . ( 1  of  1 )\n",
      "156  frames generated with label  1 .\n",
      "\n",
      "\n",
      "(156, 28, 28, 3)\n",
      "(156,)\n"
     ]
    }
   ],
   "source": [
    "file_names = ['EEGMapTest14ch.csv']\n",
    "labels = [1]\n",
    "image_size = 28\n",
    "frame_duration = 1.0\n",
    "overlap = 0.25\n",
    "X, y = make_data_pipeline(file_names,labels,image_size,frame_duration,overlap)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.imshow(X[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
